{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381211e9-8c60-4377-9d71-765527e51fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH set to: /media/baran/Disk1/Segmentation-of-Teeth-in-Panoramic\n",
      "Working directory set to: /media/baran/Disk1/Segmentation-of-Teeth-in-Panoramic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# PYTHONPATH'i ayarla\n",
    "os.environ['PYTHONPATH'] = \"/media/baran/Disk1/Segmentation-of-Teeth-in-Panoramic\"\n",
    "print(f\"PYTHONPATH set to: {os.environ['PYTHONPATH']}\")\n",
    "\n",
    "# √áalƒ±≈üma dizinini deƒüi≈ütir\n",
    "os.chdir('/media/baran/Disk1/Segmentation-of-Teeth-in-Panoramic')\n",
    "print(f\"Working directory set to: {os.getcwd()}\")\n",
    "!python src/training/train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2019c8b2-b64f-42d7-90a9-9629a9e7734b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-15 13:48:57.992696: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-15 13:48:58.006240: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-15 13:48:58.010061: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 13:48:58.021328: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 13:48:58.886553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/baran/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/media/baran/Disk1/Segmentation-of-Teeth-in-Panoramic/src/training/train.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "{'loss': 6.8047, 'grad_norm': 6.463098526000977, 'learning_rate': 5.97489539748954e-05, 'epoch': 0.04}\n",
      "{'loss': 6.4987, 'grad_norm': 5.148446559906006, 'learning_rate': 5.9497907949790794e-05, 'epoch': 0.08}\n",
      "{'loss': 6.2011, 'grad_norm': 5.147571086883545, 'learning_rate': 5.9246861924686194e-05, 'epoch': 0.13}\n",
      "{'loss': 6.0275, 'grad_norm': 5.980943202972412, 'learning_rate': 5.899581589958159e-05, 'epoch': 0.17}\n",
      "{'loss': 5.8342, 'grad_norm': 5.343048572540283, 'learning_rate': 5.8744769874476986e-05, 'epoch': 0.21}\n",
      "  2%|‚ñä                                        | 50/2390 [00:19<14:52,  2.62it/s]\n",
      "  0%|                                                    | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|‚ñà‚ñç                                          | 2/60 [00:00<00:06,  8.94it/s]\u001b[A\n",
      "  5%|‚ñà‚ñà‚ñè                                         | 3/60 [00:00<00:08,  6.34it/s]\u001b[A\n",
      "  7%|‚ñà‚ñà‚ñâ                                         | 4/60 [00:00<00:10,  5.47it/s]\u001b[A\n",
      "  8%|‚ñà‚ñà‚ñà‚ñã                                        | 5/60 [00:00<00:10,  5.08it/s]\u001b[A\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 6/60 [00:01<00:11,  4.86it/s]\u001b[A\n",
      " 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                      | 7/60 [00:01<00:11,  4.73it/s]\u001b[A\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                      | 8/60 [00:01<00:11,  4.63it/s]\u001b[A\n",
      " 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                     | 9/60 [00:01<00:11,  4.57it/s]\u001b[A\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                   | 10/60 [00:02<00:11,  4.54it/s]\u001b[A\n",
      " 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                   | 11/60 [00:02<00:10,  4.52it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 12/60 [00:02<00:10,  4.49it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 13/60 [00:02<00:10,  4.47it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 14/60 [00:02<00:10,  4.46it/s]\u001b[A\n",
      " 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                | 15/60 [00:03<00:10,  4.44it/s]\u001b[A\n",
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                               | 16/60 [00:03<00:09,  4.43it/s]\u001b[A\n",
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 17/60 [00:03<00:09,  4.43it/s]\u001b[A\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 18/60 [00:03<00:09,  4.24it/s]\u001b[A\n",
      " 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                             | 19/60 [00:04<00:09,  4.25it/s]\u001b[A\n",
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                            | 20/60 [00:04<00:09,  4.09it/s]\u001b[A\n",
      " 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                            | 21/60 [00:04<00:09,  4.14it/s]\u001b[A\n",
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 22/60 [00:04<00:09,  4.19it/s]\u001b[A\n",
      " 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 23/60 [00:05<00:09,  3.98it/s]\u001b[A\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 24/60 [00:05<00:08,  4.06it/s]\u001b[A\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 25/60 [00:05<00:08,  4.00it/s]\u001b[A\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                        | 26/60 [00:05<00:08,  3.87it/s]\u001b[A\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                       | 27/60 [00:06<00:08,  3.86it/s]\u001b[A\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                       | 28/60 [00:06<00:08,  3.77it/s]\u001b[A\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 29/60 [00:06<00:08,  3.72it/s]\u001b[A\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 30/60 [00:06<00:07,  3.76it/s]\u001b[A\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 31/60 [00:07<00:07,  3.70it/s]\u001b[A\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 32/60 [00:07<00:07,  3.68it/s]\u001b[A\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 33/60 [00:07<00:07,  3.66it/s]\u001b[A\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                  | 34/60 [00:08<00:07,  3.66it/s]\u001b[A\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 35/60 [00:08<00:06,  3.64it/s]\u001b[A\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 36/60 [00:08<00:06,  3.66it/s]\u001b[A\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 37/60 [00:08<00:06,  3.64it/s]\u001b[A\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè               | 38/60 [00:09<00:06,  3.60it/s]\u001b[A\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 39/60 [00:09<00:05,  3.61it/s]\u001b[A\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 40/60 [00:09<00:05,  3.60it/s]\u001b[A\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 41/60 [00:10<00:05,  3.54it/s]\u001b[A\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 42/60 [00:10<00:05,  3.54it/s]\u001b[A\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 43/60 [00:10<00:04,  3.56it/s]\u001b[A\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 44/60 [00:10<00:04,  3.56it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"/media/baran/Disk1/Segmentation-of-Teeth-in-Panoramic/src/training/train.py\", line 108, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2122, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2541, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2997, in _maybe_log_save_evaluate\n",
      "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 2951, in _evaluate\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3964, in evaluate\n",
      "    output = eval_loop(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 4158, in evaluation_loop\n",
      "    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 4374, in prediction_step\n",
      "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/trainer.py\", line 3625, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 823, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 811, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/transformers/models/segformer/modeling_segformer.py\", line 809, in forward\n",
      "    loss = loss_fct(upsampled_logits, labels)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py\", line 1293, in forward\n",
      "    return F.cross_entropy(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baran/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py\", line 3479, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 490.25 MiB is free. Including non-PyTorch memory, this process has 3.32 GiB memory in use. Of the allocated memory 2.48 GiB is allocated by PyTorch, and 773.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  2%|‚ñè         | 50/2390 [00:31<24:44,  1.58it/s]                               \n",
      "\n",
      "                                                                                \u001b[A"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36104a1-c05a-4504-8883-e0b60c7ae27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
